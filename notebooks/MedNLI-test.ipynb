{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc26aa3-110f-455a-bd72-a305131e53e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import datasets\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f642d6-ee32-4804-b963-23444192e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_header_in_csv(file, fields: list):\n",
    "    with open(file, 'w', encoding='utf-8-sig', newline='') as csvFile:\n",
    "        writer = csv.DictWriter(\n",
    "            csvFile, fieldnames=fields, delimiter=',', escapechar='\\\\')\n",
    "\n",
    "        writer.writeheader()\n",
    "\n",
    "def append_in_csv(file, data: dict):\n",
    "    with open(file, 'a', encoding='utf-8-sig', newline='') as csvFile:\n",
    "        writer = csv.DictWriter(\n",
    "            csvFile, fieldnames=data.keys(), delimiter=',', escapechar='\\\\')\n",
    "\n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b3b54b-4e05-465d-9ffc-be616bee6f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(data):\n",
    "    labels = {'contradiction': 0, 'entailment': 1, 'neutral': 2}\n",
    "    key = data['gold_label']\n",
    "    data = {'label': labels[key]}\n",
    "\n",
    "    return data\n",
    "\n",
    "def tokenize(data):\n",
    "    data = tokenizer(\n",
    "        data['sentence1'],\n",
    "        data['sentence2'],\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "def compute_metrics(y):\n",
    "    logits, labels = y\n",
    "    preds = logits.argmax(axis=1)\n",
    "    metrics = 'precision', 'recall', 'f1'\n",
    "    result = {}\n",
    "\n",
    "    result.update(\n",
    "        load('accuracy').compute(\n",
    "            predictions=preds,\n",
    "            references=labels\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for metric in metrics:\n",
    "        result.update(\n",
    "            load(metric).compute(\n",
    "                predictions=preds,\n",
    "                references=labels, \n",
    "                average='macro'\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ac3e62-af5b-4cb4-ab57-9597724f5927",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_DATA = 'mednli'\n",
    "NAME_MODELS = (\n",
    "    'cross-encoder/nli-deberta-v3-base',\n",
    "    'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext',\n",
    "    'sentence-transformers/stsb-roberta-base-v2',\n",
    "    'microsoft/deberta-v3-base'\n",
    ")\n",
    "\n",
    "PATH_BASE = Path('../')\n",
    "PATH_MODELS = PATH_BASE/'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f707bcd-c32f-445b-afc9-0ae32096ccd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cross-encoder/nli-deberta-v3-base': ['mednli_nli-deberta-v3-base_1675166374',\n",
       "  'mednli_nli-deberta-v3-base_1675180105',\n",
       "  'mednli_nli-deberta-v3-base_1675241894'],\n",
       " 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext': ['mednli_BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext_1675257677',\n",
       "  'mednli_BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext_1675307496'],\n",
       " 'sentence-transformers/stsb-roberta-base-v2': ['mednli_stsb-roberta-base-v2_1675258266'],\n",
       " 'microsoft/deberta-v3-base': ['mednli_deberta-v3-base_1675300074']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = os.listdir(PATH_MODELS)\n",
    "modelDict = {}\n",
    "\n",
    "for M in NAME_MODELS:\n",
    "    modelDict[M] = []\n",
    "    key = M.split('/')[-1]\n",
    "    for P in models:\n",
    "        if not (NAME_DATA in P and os.path.isdir(PATH_MODELS/P)):\n",
    "            continue\n",
    "        if key == ''.join(P.split('_')[1:][:-1]):\n",
    "            modelDict[M].append(P)\n",
    "\n",
    "modelDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309c8389-77ae-4cdb-9b38-7186314b30a5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration mednli_source-03971dcdb462331f\n",
      "Found cached dataset mednli (C:/Users/wowns/.cache/huggingface/datasets/bigbio___mednli/mednli_source-03971dcdb462331f/1.0.0/f18e31046c4bf68bbdc505fbf3dab924a4ea60082a7f41346e3a84b54f453535)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for '..\\models\\mednli_nli-deberta-v3-base_1675166374\\weights'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '..\\models\\mednli_nli-deberta-v3-base_1675166374\\weights' is the correct path to a directory containing all relevant files for a DebertaV2TokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 16\u001b[0m\n\u001b[0;32m     10\u001b[0m PATH_MODEL \u001b[38;5;241m=\u001b[39m PATH_MODELS\u001b[38;5;241m/\u001b[39mNAME_MODEL\n\u001b[0;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     13\u001b[0m     PATH_MODEL\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     14\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     15\u001b[0m )     \n\u001b[1;32m---> 16\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPATH_MODEL\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweights\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(label_encode)\n\u001b[0;32m     22\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(tokenize, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:676\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    674\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_fast\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    677\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\nlp\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1788\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1782\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load following files from cache: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munresolved_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and cannot check if these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1785\u001b[0m     )\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m-> 1788\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   1789\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1790\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1791\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1792\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1793\u001b[0m     )\n\u001b[0;32m   1795\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   1796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for '..\\models\\mednli_nli-deberta-v3-base_1675166374\\weights'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '..\\models\\mednli_nli-deberta-v3-base_1675166374\\weights' is the correct path to a directory containing all relevant files for a DebertaV2TokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "for NAME_MODEL, models in modelDict.items():\n",
    "    dataset = datasets.load_dataset(\n",
    "        'bigbio/mednli', \n",
    "        'mednli_source',\n",
    "        data_dir=PATH_BASE/'data',\n",
    "        split='test'\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(NAME_MODEL)\n",
    "\n",
    "    dataset = dataset.map(label_encode)\n",
    "    dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "    for NAME_MODEL in models:\n",
    "        PATH_MODEL = PATH_MODELS/NAME_MODEL\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            PATH_MODEL/'weights', \n",
    "            local_files_only=True\n",
    "        )\n",
    "\n",
    "        # cols = list(dataset.column_names)\n",
    "        cols = ['sentence1', 'sentence2', 'label', 'pred']\n",
    "        write_header_in_csv(PATH_MODEL/'pred.csv', cols)\n",
    "        \n",
    "        model.eval()\n",
    "        for data in dataset:\n",
    "            features = tokenizer(\n",
    "                [[data['sentence1'], data['sentence2']]],\n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                score = model(**features).logits\n",
    "                \n",
    "            pred = score.argmax(dim=1).numpy()\n",
    "            data = dict(\n",
    "                sentence1=data['sentence1'],\n",
    "                sentence2=data['sentence2'],\n",
    "                label=data['label'],\n",
    "                pred=pred[0]\n",
    "            )\n",
    "\n",
    "            append_in_csv(PATH_MODEL/'pred.csv', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3adcca1-78ce-4be6-b23d-d998a2885558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "Both warmup_ratio and warmup_steps given, warmup_steps will override any effect of warmup_ratio during training\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: sentence1, sentence1_binary_parse, pairID, sentence2_parse, gold_label, sentence1_parse, sentence2_binary_parse, sentence2. If sentence1, sentence1_binary_parse, pairID, sentence2_parse, gold_label, sentence1_parse, sentence2_binary_parse, sentence2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1422\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 23:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.499047</td>\n",
       "      <td>0.824191</td>\n",
       "      <td>0.824744</td>\n",
       "      <td>0.824191</td>\n",
       "      <td>0.824258</td>\n",
       "      <td>1464.8049</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eval_loss  eval_accuracy  eval_precision  eval_recall   eval_f1  \\\n",
       "0   0.499047       0.824191        0.824744     0.824191  0.824258   \n",
       "\n",
       "   eval_runtime  eval_samples_per_second  eval_steps_per_second  \n",
       "0     1464.8049                    0.971                  0.031  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(PATH_MODEL/'args.json', 'r') as f:\n",
    "    args = json.load(f)\n",
    "    \n",
    "args = TrainingArguments(**args)\n",
    "trainer = Trainer(model, args, compute_metrics=compute_metrics)\n",
    "\n",
    "result = trainer.evaluate(dataset)\n",
    "df = pd.DataFrame.from_dict([result])\n",
    "\n",
    "df.to_csv(PATH_MODEL/'result.csv', index=False, encoding='utf-8')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "730c2570-9c9c-437b-a052-2a7ea2b929a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8242])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(result) / len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc33e16-81ca-4dab-bb17-b06581e082db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
